{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhWk_ZHP3h4A"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-pinecone pinecone-notebooks langchain-google-genai langchain-core langchain_community docx2txt pypdf sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVeFEEarjgJa"
      },
      "source": [
        "# Step 1: Initialize Pinecone and Set Up API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSGGHutHzflf"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS4RGiOk0wFB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"basic-rag-system\"\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq--9OHel6bw"
      },
      "source": [
        "# Step 2: Use LangChain for RAG Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKzIkMgSj8XE"
      },
      "source": [
        "## 1. Set Up Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9WmRW9B1gGO"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpWzL0zlkFoe"
      },
      "source": [
        "## 2. Set Up Document Loader and Text Spliter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7zAUMeMLsIh"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import List\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgWFrvwsLjUw"
      },
      "outputs": [],
      "source": [
        "# Function to load documents from a folder\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      if filename.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "      elif filename.endswith(\".docx\"):\n",
        "        loader = Docx2txtLoader(file_path)\n",
        "      else:\n",
        "        print(f\"Unsupported file type: {filename}\")\n",
        "        continue\n",
        "      documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "# Load documents from a folder\n",
        "folder_path = \"/content/docs\"\n",
        "documents = load_documents(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0iC_hHCM5kX",
        "outputId": "78151712-be08-441e-db39-86096cf52aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 41 documents from the folder.\n",
            "Split the documents into 41 chunks.\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 700,\n",
        "    chunk_overlap = 40,\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4a_H6cKkPYW"
      },
      "source": [
        "## 3. Setup Vector Store, Embed and Store Documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UZbuK2g2weX"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtLmcCTk38F5"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "uuids = [str(uuid4()) for _ in range(len(splits))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1gwWmKfo4CvU",
        "outputId": "9c561637-b974-4748-c6de-7606f708d64e"
      },
      "outputs": [],
      "source": [
        "vector_store.add_documents(documents=splits, ids=uuids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6RcJ48YlRdz"
      },
      "source": [
        "## 4. Set Up Data Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypHk96In3w74",
        "outputId": "fe2b692d-5928-4b66-df41-ebe6490562c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Transfer Learning Fine-Tuning\n",
            "Uses a pre-trained model for new tasks. Adapts a pre-trained model to a specific task.\n",
            "Generalized knowledge transfer. Task-specific performance improvement.\n",
            "Retrains only the final layers. Updates specific or all model weights.\n",
            "Minimal new data. Requires labeled task data.\n",
            "Limited customization. Fully customizable.\n",
            "Lower computational cost, as fewer parameters are trained. Higher cost, depending on task complexity.\n",
            "(c) 2024 Data Science Dojo. No reproduction permitted.\n",
            "Fine Tuning vs. Transfer Learning\n",
            "How are they different? [{'page': 11.0, 'source': '/content/docs/Updated Fine-tuning .pdf'}]\n",
            "* LLM Bootamp\n",
            "(c) 2024 Data Science Dojo. No reproduction permitted.\n",
            "Fine Tuning vs.\n",
            "Transfer Learning\n",
            "Transfer learning\n",
            "Fine tuning\n",
            "Key differences between fine tunign and\n",
            "transfer learning [{'page': 5.0, 'source': '/content/docs/Updated Fine-tuning .pdf'}]\n"
          ]
        }
      ],
      "source": [
        "# Data Retrieval\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    \"What is the difference between fine-tuning and trasfer learning?\",\n",
        "    k=2,\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAibpNx4lf_T"
      },
      "source": [
        "# Step 3: Set Up Google Gemini Flash Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YHIULiv59tq"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwzmyow5lsh-"
      },
      "source": [
        "# Step 4: Combine Retriever and LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbBjaWh_HyHw"
      },
      "outputs": [],
      "source": [
        "def answer_to_user(query: str):\n",
        "\n",
        "  #Vector Search\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "\n",
        "  final_answer = llm.invoke(f\"ANSWER THIS USER QUERY: {query} USING THIS CONTEXT: {vector_results}\")\n",
        "\n",
        "  return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_gGNCculw4B"
      },
      "source": [
        "# Step 5: Query the RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PzP29OC7iNAU",
        "outputId": "ea85de52-1eaf-42d0-971d-2bb9e89f0831"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Fine tuning is a process that unfreezes and retrains some or all layers of a pre-trained model.  This adjusts the model to a specific dataset and its features.  It's used when a moderate to large dataset is available, the task differs from the pre-trained model's original task, and higher customization is needed.\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = answer_to_user(\"What is Fine tuning?\")\n",
        "answer.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Igk-QsEJM8-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
